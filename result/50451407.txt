前两日，在微博上说：“到今天为止，我至少亏欠了3篇文章待写：1、KD树；2、神经网络；3、编程艺术第28章。你看到，blog内的文章与你于别处所见的任何都不同。于是，等啊等，等一台电脑，只好等待..”。得益于田，借了我一台电脑（借他电脑的时候，我连表示感谢，他说“能找到工作全靠你的博客，这点儿小忙还说，不地道”，有的时候，稍许感受到受人信任也是一种压力，愿我不辜负大家对我的信任），于是今天开始Top
10AlgorithmsinDataMining系列第三篇文章，即本文「从K近邻算法谈到KD树、SIFT&#43;BBF算法」的创作。
一个人坚持自己的兴趣是比较难的，因为太多的人太容易为外界所动了，而尤其当你无法从中得到多少实际性的回报时，所幸，我能一直坚持下来。毕达哥拉斯学派有句名言：“万物皆数”，最近读完「微积分概念发展史」后也感受到了这一点。同时，从算法到数据挖掘、机器学习，再到数学，其中每一个领域任何一个细节都&#20540;得探索终生，或许，这就是“终生为学”的意思。
本文各部分内容分布如下：
同时，你将看到，K近邻算法同本系列的前两篇文章所讲的决策树分类贝叶斯分类，及支持向量机SVM一样，也是用于解决分类问题的算法，
而本数据挖掘十大算法系列也会按照分类，聚类，关联分析，预测回归等问题依次展开阐述。
OK，行文仓促，本文若有任何漏洞，问题或者错误，欢迎朋友们随时不吝指正，各位的批评也是我继续写下去的动力之一。感谢。

何谓K近邻算法，即K-NearestNeighboralgorithm，简称KNN算法，单从名字来猜想，可以简单粗暴的认为是：K个最近的邻居，当K=1时，算法便成了最近邻算法，即寻找最近的那个邻居。为何要找邻居？打个比方来说，假设你来到一个陌生的村庄，现在你要找到与你有着相&#20284;特征的人群融入他们，所谓入伙。
用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。根据这个说法，咱们来看下引自维基百科上的一幅图：

如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。



我们常说，物以类聚，人以群分，判别一个人是一个什么样品质特征的人，常常可以从他/她身边的朋友入手，所谓观其友，而识其人。我们不是要判别上图中那个绿色的圆是属于哪一类数据么，好说，从它的邻居下手。但一次性看多少个邻居呢？从上图中，你还能看到：
于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。
上文第一节，我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。但有的读者可能就有疑问了，我是要找邻居，找相&#20284;性，怎么又跟距离扯上关系了？
这是因为特征空间中两个实例点的距离和反应出两个实例点之间的相&#20284;性程度。K近邻模型的特征空间一般是n维实数向量空间，使用的距离可以使欧式距离，也是可以是其它距离，既然扯到了距离，下面就来具体阐述下都有哪些距离度量的表示法，权当扩展。
(1)二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：


其上，二维平面上两点欧式距离，代码可以如下编写：

(1)在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：

(2)
两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦

类&#20284;的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类&#20284;于夹角余弦的概念来衡量它们间的相&#20284;程度，即：

夹角余弦取&#20540;范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大&#20540;1，当两个向量的方向完全相反夹角余弦取最小&#20540;-1。
通常情况下通过以下取&#20540;范围判断变量的相关强度：

相关系数



0.8-1.0




极强相关

















0.6-0.8



强相关

















0.4-0.6



中等程度相关

















0.2-0.4



弱相关

















0.0-0.2



极弱相关或无相关
rubyist：皮尔逊相关系数理解有两个角度
(4)皮尔逊相关的约束条件
除了上述1.2节如何定义邻居的问题之外，还有一个选择多少个邻居，即K&#20540;定义为多大的问题。不要小看了这个K&#20540;选择问题，因为它对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：

之前blog内曾经介绍过SIFT特征匹配算法，特征点匹配和数据库查、图像检索本质上是同一个问题，都可以归结为一个通过距离函数在高维矢量之间进行相&#20284;性检索的问题，如何快速而准确地找到查询点的近邻，不少人提出了很多高维空间索引结构和近&#20284;查询的算法。
一般说来，索引结构中相&#20284;性查询有两种基本的方式：
同样，针对特征点匹配也有两种方法：
而关于R树本blog内之前已有介绍(同时，关于基于R树的最近邻查找，还可以看下这篇文章：http://blog.sina.com.cn/s/blog_72e1c7550101dsc3.html)，本文着重介绍k-d树。
1975年，来自斯坦福大学的JonLouisBentley在ACM杂志上发表的一篇论文：MultidimensionalBinarySearchTreesUsedforAssociativeSearching中正式提出和阐述的了如下图形式的把空间划分为多个部分的k-d树。

Kd-树是K-dimensiontree的缩写，是对数据点在k维空间（如二维(x，y)，三维(x，y，z)，k维(x1，y，z..)）中划分的一种数据结构，主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。本质上说，Kd-树就是一种平衡二叉树。
首先必须搞清楚的是，k-d树是一种空间划分树，说白了，就是把整个空间划分为特定的几个部分，然后在特定空间的部分内进行相关搜索操作。想像一个三维(多维有点为难你的想象力了)空间，kd树按照一定的划分规则把这个三维空间划分了多个空间，如下图所示：

kd树构建的伪代码如下图所示：

再举一个简单直观的实例来介绍k-d树构建算法。假设有6个二维数据点{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，数据点位于二维空间内，如下图所示。为了能有效的找到最近邻，k-d树采用分而治之的思想，即将整个空间划分为几个小部分，首先，粗黑线将空间一分为二，然后在两个子空间中，细黑直线又将整个空间划分为四部分，最后虚黑直线将这四部分进一步划分。

6个二维数据点{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}构建kd树的具体步骤为：
与此同时，经过对上面所示的空间划分之后，我们可以看出，点(7,2)可以为根结点，从根结点出发的两条红粗斜线指向的(5,4)和(9,6)则为根结点的左右子结点，而(2,3)，(4,7)则为(5,4)的左右孩子(通过两条细红斜线相连)，最后，(8,1)为(9,6)的左孩子(通过细红斜线相连)。如此，便形成了下面这样一棵k-d树：

k-d树的数据结构

针对上表给出的kd树的数据结构，转化成具体代码如下所示(注，本文以下代码分析基于RobHess维护的sift库)：
也就是说，如之前所述，kd树中，kd代表k-dimension，每个节点即为一个k维的点。每个非叶节点可以想象为一个分割超平面，用垂直于坐标轴的超平面将空间分为两个部分，这样递归的从根节点不停的划分，直到没有实例为止。经典的构造k-dtree的规则如下：
对于n个实例的k维数据来说，建立kd-tree的时间复杂度为O(k*n*logn)。
以下是构建k-d树的代码：
上面的涉及初始化操作的两个函数kd_node_init，及expand_kd_node_subtree代码分别如下所示：
构建完kd树之后，如今进行最近邻搜索呢？从下面的动态gif图中，你是否能看出些许端倪呢？

k-d树算法可以分为两大部分，除了上部分有关k-d树本身这种数据结构建立的算法，另一部分是在建立的k-d树上各种诸如插入，删除，查找(最邻近查找)等操作涉及的算法。下面，咱们依次来看kd树的插入、删除、查找操作。

元素插入到一个K-D树的方法和二叉检索树类&#20284;。本质上，在偶数层比较x坐标&#20540;，而在奇数层比较y坐标&#20540;。当我们到达了树的底部，（也就是当一个空指针出现），我们也就找到了结点将要插入的位置。生成的K-D树的形状依赖于结点插入时的顺序。给定N个点，其中一个结点插入和检索的平均代价是O(log2N)。
下面4副图(来源：中国地质大学电子课件)说明了插入顺序为(a)Chicago,(b)Mobile,(c)Toronto,and(d)Buffalo，建立空间K-D树的示例：
应该清楚，这里描述的插入过程中，每个结点将其所在的平面分割成两部分。因比，Chicago将平面上所有结点分成两部分，一部分所有的结点x坐标&#20540;小于35，另一部分结点的x坐标&#20540;大于或等于35。同样Denver将所有x坐标&#20540;小于35的结点以分成两部分，一部分结点的Y坐标&#20540;是小于45，另一部分结点的Y坐标&#20540;大于或等于45。

上述两次实例表明，当查询点的邻域与分割超平面两侧空间交割时，需要查找另一侧子空间，导致检索过程复杂，效率下降。



研究表明N个节点的K维k-d树搜索过程时间复杂度为：tworst=O（kN1-1/k）。
同时，以上为了介绍方便，讨论的是二维或三维情形。但在实际的应用中，如SIFT特征矢量128维，SURF特征矢量64维，维度都比较大，直接利用k-d树快速检索（维数不超过20）的性能急剧下降，几乎接近贪婪线性扫描。假设数据集的维数为D，一般来说要求数据的规模N满足N»2D，才能达到高效的搜索。所以这就引出了一系列对k-d树算法的改进：BBF算法，和一系列M树、VP树、MVP树等高维空间索引树(下文2.6节kd树近邻搜索算法的改进：BBF算法，与2.7节球树、M树、VP树、MVP树)。
咱们顺着上一节的思路，参考统计学习方法一书上的内容，再来总结下kd树的最近邻搜索算法：
如果实例点是随机分布的，那么kd树搜索的平均计算复杂度是O（NlogN），这里的N是训练实例树。所以说，kd树更适用于训练实例数远大于空间维数时的k近邻搜索，当空间维数接近训练实例数时，它的效率会迅速下降，一降降到“解放前”：线性扫描的速度。
也正因为上述k最近邻搜索算法的第4个步骤中的所述：“回退到根结点时，搜索结束”，每个最近邻点的查询比较完成过程最终都要回退到根结点而结束，而导致了许多不必要回溯访问和比较到的结点，这些多余的损耗在高维度数据查找的时候，搜索效率将变得相当之地下，那有什么办法可以改进这个原始的kd树最近邻搜索算法呢？
从上述标准的kd树查询过程可以看出其搜索过程中的“回溯”是由“查询路径”决定的，并没有考虑查询路径上一些数据点本身的一些性质。一个简单的改进思路就是将“查询路径”上的结点进行排序，如按各自分割超平面（也称bin）与查询点的距离排序，也就是说，回溯检查总是从优先级最高（BestBin）的树结点开始。
针对此BBF机制，读者Feng&amp;书童点评道：
如此，就引出了本节要讨论的kd树最近邻搜索算法的改进：BBF（Best-Bin-First）查询算法，它是由发明sift算法的DavidLowe在1997的一篇文章中针对高维数据提出的一种近&#20284;算法，此算法能确保优先检索包含最近邻点可能性较高的空间，此外，BBF机制还设置了一个运行超时限定。采用了BBF查询机制后，kd树便可以有效的扩展到高维数据集上。
伪代码如下图所示（图取自图像局部不变特性特征与描述一书）：

还是以上面的查询（2,4.5）为例，搜索的算法流程为：

咱们来针对上文内容总结回顾下，针对下面这样一棵kd树：

现要找它的最近邻。
通过上文2.5节，总结来说，我们已经知道：
1、为了找到一个给定目标点的最近邻，需要从树的根结点开始向下沿树找出目标点所在的区域，如下图所示，给定目标点，用星号标示，我们&#20284;乎一&#30524;看出，有一个点离目标点最近，因为它落在以目标点为圆心以较小长度为半径的虚线圆内，但为了确定是否可能还村庄一个最近的近邻，我们会先检查叶节点的同胞结点，然叶节点的同胞结点在图中所示的阴影部分，虚线圆并不与之相交，所以确定同胞叶结点不可能包含更近的近邻。

2、于是我们回溯到父节点，并检查父节点的同胞结点，父节点的同胞结点覆盖了图中所有横线X轴上的区域。因为虚线圆与右上方的矩形(KD树把二维平面划分成一个一个矩形)相交...
如上，我们看到，KD树是可用于有效寻找最近邻的一个树结构，但这个树结构其实并不完美，当处理不均匀分布的数据集时便会呈现出一个基本冲突：既邀请树有完美的平衡结构，又要求待查找的区域近&#20284;方形，但不管是近&#20284;方形，还是矩形，甚至正方形，都不是最好的使用形状，因为他们都有角。

什么意思呢？就是说，在上图中，如果黑色的实例点离目标点星点再远一点，那么势必那个虚线圆会如红线所示那样扩大，以致与左上方矩形的右下角相交，既然相交了，那么势必又必须检查这个左上方矩形，而实际上，最近的点离星点的距离很近，检查左上方矩形区域已是多余。于此我们看见，KD树把二维平面划分成一个一个矩形，但矩形区域的角却是个难以处理的问题。
解决的方案就是使用如下图所示的球树：


使用球树找出给定目标点的最近邻方法是，首先自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最靠近的点，这将确定出目标点距离它的最近邻点的一个上限&#20540;，然后跟KD树查找一样，检查同胞结点，如果目标点到同胞结点中心的距离超过同胞结点的半径与当前的上限&#20540;之和，那么同胞结点里不可能存在一个更近的点；否则的话，必须进一步检查位于同胞结点以下的子树。
如下图，目标点还是用一个星表示，黑色点是当前已知的的目标点的最近邻，灰色球里的所有内容将被排除，因为灰色球的中心点离的太远，所以它不可能包含一个更近的点，像这样，递归的向树的根结点进行回溯处理，检查所有可能包含一个更近于当前上限&#20540;的点的球。

球树是自上而下的建立，和KD树一样，根本问题就是要找到一个好的方法将包含数据点集的球分裂成两个，在实践中，不必等到叶子结点只有两个胡数据点时才停止，可以采用和KD树一样的方法，一旦结点上的数据点打到预先设置的最小数量时，便可提前停止建树过程。
也就是上面所述，先从球中选择一个离球的中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个上，然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径。这种方法的优点是分裂一个包含n个殊绝点的球的成本只是随n呈线性增加(注：本小节内容主要来自参考条目19：数据挖掘实用机器学习技术，[新西兰]Ian
H.Witten著，第4章4.7节)。
高维特征向量的距离索引问题是基于内容的图像检索的一项关键技术，目前经常采用的解决办法是首先对高维特征空间做降维处理，然后采用包括四叉树、kd树、R树族等在内的主流多维索引结构，这种方法的出发点是：目前的主流多维索引结构在处理维数较低的情况时具有比较好的效率，但对于维数很高的情况则显得力不从心(即所谓的维数危机)。
实验结果表明当特征空间的维数超过20的时候，效率明显降低，而可视化特征往往采用高维向量描述，一般情况下可以达到10^2的量级，甚至更高。在表示图像可视化特征的高维向量中各维信息的重要程度是不同的，通过降维技术去除属于次要信息的特征向量以及相关性较强的特征向量，从而降低特征空间的维数，这种方法已经得到了一些实际应用。
然而这种方法存在不足之处采用降维技术可能会导致有效信息的损失，尤其不适合于处理特征空间中的特征向量相关性很小的情况。另外主流的多维索引结构大都针对欧氏空间，设计需要利用到欧氏空间的几何性质，而图像的相&#20284;性计算很可能不限于基于欧氏距离。这种情况下人们越来越关注基于距离的度量空间高维索引结构可以直接应用于高维向量相&#20284;性查询问题。
度量空间中对象之间的距离度量只能利用三角不等式性质，而不能利用其他几何性质。向量空间可以看作由实数坐标串组成的特殊度量空间，目前针对度量空间的高维索引问题提出的索引结构有很多种大致可以作如下分类，如下图所示：

读者点评：
更多内容请参见论文1：DISTANCE-BASEDINDEXINGFORHIGH-DIMENSIONALMETRICSPACES，作者：TolgaBozkaya&amp;MeralOzsoyoglu，及论文2：基于度量空间高维索引结构VP-tree及MVP-tree的图像检索，王志强，甘国辉，程起敏。
当然，如果你觉得上述论文还不够满足你胃口的话，这里有一大堆nearestneighboralgorithms相关的论文可供你看：http://scholar.google.com.hk/scholar?q=nearest&#43;neighbor&#43;algorithms&amp;btnG=&amp;hl=zh-CN&amp;as_sdt=0&amp;as_vis=1（其中，这篇可以看下：Spill-Trees，An
investigationofpracticalapproximatenearestneighboralgorithms）。

之前本blog内阐述过图像特征匹配SIFT算法，写过五篇文章，这五篇文章分别为：
不熟悉SIFT算法相关概念的可以看上述几篇文章，这里不再做赘述。与此同时，本文此部分也作为十五个经典算法研究系列里SIFT算法的九之四续。
OK，我们知道，在sift算法中，给定两幅图片图片，若要做特征匹配，一般会先提取出图片中的下列相关属性作为特征点：
函数一的声明：
函数一的实现：
函数二的声明：
函数二的实现：
上文中一直在讲最近邻问题，也就是说只找最近的那唯一一个邻居，但如果现实中需要我们找到k个最近的邻居。该如何做呢？对的，之前blog内曾相近阐述过寻找最小的k个数的问题，显然，寻找k个最近邻与寻找最小的k个数的问题如出一辙。
回忆下寻找k个最小的数中关于构造大顶堆的解决方案：
“寻找最小的k个树，更好的办法是维护k个元素的最大堆，即用容量为k的最大堆存储最先遍历到的k个数，并假设它们即是最小的k个数，建堆费时O（k）后，有k1&lt;k2&lt;...&lt;kmax（kmax设为大顶堆中最大元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，x&lt;kmax，更新堆（用时logk），否则不更新堆。这样下来，总费时O（k&#43;（n-k）*logk）=O（n*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。”
根据上述方法，咱们来写大顶堆最小优先级队列相关代码实现：

原理在上文第二部分已经阐述明白，结合大顶堆找最近的K个近邻思想，相关主体代码如下：
依据上述函数kdtree_bbf_knn从上而下看下来，注意几点：
1、上述函数kdtree_bbf_knn中，explore_to_leaf的代码如下：
2、上述查找函数kdtree_bbf_knn中的参数k可调，代表的是要查找近邻的个数，即numberofneighborstofind，在sift特征匹配中，k一般取2
3、上述函数kdtree_bbf_knn中“bbf_data-&gt;d=descr_dist_sq(feat,tree_feat);//计算两个关键点描述器差平方和”，使用的计算方法是本文第一部分1.2节中所述的欧氏距离。
之前试了下sift&#43;KD&#43;BBF算法，用两幅不同的图片做了下匹配（当然，运行结果显示是不匹配的），效果还不错：http://weibo.com/1580904460/yDmzAEwcV#1348475194313。

“编译的过程中，直接用的VS2010&#43;opencv（并没下gsl）。2012.09.24”。....
本文完整源码有pudn帐号的朋友可以前去这里下载：http://www.pudn.com/downloads340/sourcecode/graph/texture_mapping/detail1486667.html（没有pudn账号的同学请加群：169056165，至群共享下载，验证信息：sift）。感谢诸位。

前两日，在微博上说：“到今天为止，我至少亏欠了3篇文章待写：1、KD树；2、神经网络；3、编程艺术第28章。你看到，blog内的文章与你于别处所见的任何都不同。于是，等啊等，等一台电脑，只好等待..”。得益于田，借了我一台电脑（借他电脑的时候，我连表示感谢，他说“能找到工作全靠你的博客，这点儿小忙还说，不地道”，有的时候，稍许感受到受人信任也是一种压力，愿我不辜负大家对我的信任），于是今天开始Top
10AlgorithmsinDataMining系列第三篇文章，即本文「从K近邻算法谈到KD树、SIFT&#43;BBF算法」的创作。
一个人坚持自己的兴趣是比较难的，因为太多的人太容易为外界所动了，而尤其当你无法从中得到多少实际性的回报时，所幸，我能一直坚持下来。毕达哥拉斯学派有句名言：“万物皆数”，最近读完「微积分概念发展史」后也感受到了这一点。同时，从算法到数据挖掘、机器学习，再到数学，其中每一个领域任何一个细节都&#20540;得探索终生，或许，这就是“终生为学”的意思。
本文各部分内容分布如下：
同时，你将看到，K近邻算法同本系列的前两篇文章所讲的决策树分类贝叶斯分类，及支持向量机SVM一样，也是用于解决分类问题的算法，
而本数据挖掘十大算法系列也会按照分类，聚类，关联分析，预测回归等问题依次展开阐述。
OK，行文仓促，本文若有任何漏洞，问题或者错误，欢迎朋友们随时不吝指正，各位的批评也是我继续写下去的动力之一。感谢。

何谓K近邻算法，即K-NearestNeighboralgorithm，简称KNN算法，单从名字来猜想，可以简单粗暴的认为是：K个最近的邻居，当K=1时，算法便成了最近邻算法，即寻找最近的那个邻居。为何要找邻居？打个比方来说，假设你来到一个陌生的村庄，现在你要找到与你有着相&#20284;特征的人群融入他们，所谓入伙。
用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。根据这个说法，咱们来看下引自维基百科上的一幅图：

如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。



我们常说，物以类聚，人以群分，判别一个人是一个什么样品质特征的人，常常可以从他/她身边的朋友入手，所谓观其友，而识其人。我们不是要判别上图中那个绿色的圆是属于哪一类数据么，好说，从它的邻居下手。但一次性看多少个邻居呢？从上图中，你还能看到：
于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。
上文第一节，我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。但有的读者可能就有疑问了，我是要找邻居，找相&#20284;性，怎么又跟距离扯上关系了？
这是因为特征空间中两个实例点的距离和反应出两个实例点之间的相&#20284;性程度。K近邻模型的特征空间一般是n维实数向量空间，使用的距离可以使欧式距离，也是可以是其它距离，既然扯到了距离，下面就来具体阐述下都有哪些距离度量的表示法，权当扩展。
(1)二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：


其上，二维平面上两点欧式距离，代码可以如下编写：

(1)在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：

(2)
两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦

类&#20284;的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类&#20284;于夹角余弦的概念来衡量它们间的相&#20284;程度，即：

夹角余弦取&#20540;范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大&#20540;1，当两个向量的方向完全相反夹角余弦取最小&#20540;-1。
通常情况下通过以下取&#20540;范围判断变量的相关强度：

相关系数



0.8-1.0




极强相关

















0.6-0.8



强相关

















0.4-0.6



中等程度相关

















0.2-0.4



弱相关

















0.0-0.2



极弱相关或无相关
rubyist：皮尔逊相关系数理解有两个角度
(4)皮尔逊相关的约束条件
除了上述1.2节如何定义邻居的问题之外，还有一个选择多少个邻居，即K&#20540;定义为多大的问题。不要小看了这个K&#20540;选择问题，因为它对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：

之前blog内曾经介绍过SIFT特征匹配算法，特征点匹配和数据库查、图像检索本质上是同一个问题，都可以归结为一个通过距离函数在高维矢量之间进行相&#20284;性检索的问题，如何快速而准确地找到查询点的近邻，不少人提出了很多高维空间索引结构和近&#20284;查询的算法。
一般说来，索引结构中相&#20284;性查询有两种基本的方式：
同样，针对特征点匹配也有两种方法：
而关于R树本blog内之前已有介绍(同时，关于基于R树的最近邻查找，还可以看下这篇文章：http://blog.sina.com.cn/s/blog_72e1c7550101dsc3.html)，本文着重介绍k-d树。
1975年，来自斯坦福大学的JonLouisBentley在ACM杂志上发表的一篇论文：MultidimensionalBinarySearchTreesUsedforAssociativeSearching中正式提出和阐述的了如下图形式的把空间划分为多个部分的k-d树。

Kd-树是K-dimensiontree的缩写，是对数据点在k维空间（如二维(x，y)，三维(x，y，z)，k维(x1，y，z..)）中划分的一种数据结构，主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。本质上说，Kd-树就是一种平衡二叉树。
首先必须搞清楚的是，k-d树是一种空间划分树，说白了，就是把整个空间划分为特定的几个部分，然后在特定空间的部分内进行相关搜索操作。想像一个三维(多维有点为难你的想象力了)空间，kd树按照一定的划分规则把这个三维空间划分了多个空间，如下图所示：

kd树构建的伪代码如下图所示：

再举一个简单直观的实例来介绍k-d树构建算法。假设有6个二维数据点{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，数据点位于二维空间内，如下图所示。为了能有效的找到最近邻，k-d树采用分而治之的思想，即将整个空间划分为几个小部分，首先，粗黑线将空间一分为二，然后在两个子空间中，细黑直线又将整个空间划分为四部分，最后虚黑直线将这四部分进一步划分。

6个二维数据点{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}构建kd树的具体步骤为：
与此同时，经过对上面所示的空间划分之后，我们可以看出，点(7,2)可以为根结点，从根结点出发的两条红粗斜线指向的(5,4)和(9,6)则为根结点的左右子结点，而(2,3)，(4,7)则为(5,4)的左右孩子(通过两条细红斜线相连)，最后，(8,1)为(9,6)的左孩子(通过细红斜线相连)。如此，便形成了下面这样一棵k-d树：

k-d树的数据结构

针对上表给出的kd树的数据结构，转化成具体代码如下所示(注，本文以下代码分析基于RobHess维护的sift库)：
也就是说，如之前所述，kd树中，kd代表k-dimension，每个节点即为一个k维的点。每个非叶节点可以想象为一个分割超平面，用垂直于坐标轴的超平面将空间分为两个部分，这样递归的从根节点不停的划分，直到没有实例为止。经典的构造k-dtree的规则如下：
对于n个实例的k维数据来说，建立kd-tree的时间复杂度为O(k*n*logn)。
以下是构建k-d树的代码：
上面的涉及初始化操作的两个函数kd_node_init，及expand_kd_node_subtree代码分别如下所示：
构建完kd树之后，如今进行最近邻搜索呢？从下面的动态gif图中，你是否能看出些许端倪呢？

k-d树算法可以分为两大部分，除了上部分有关k-d树本身这种数据结构建立的算法，另一部分是在建立的k-d树上各种诸如插入，删除，查找(最邻近查找)等操作涉及的算法。下面，咱们依次来看kd树的插入、删除、查找操作。

元素插入到一个K-D树的方法和二叉检索树类&#20284;。本质上，在偶数层比较x坐标&#20540;，而在奇数层比较y坐标&#20540;。当我们到达了树的底部，（也就是当一个空指针出现），我们也就找到了结点将要插入的位置。生成的K-D树的形状依赖于结点插入时的顺序。给定N个点，其中一个结点插入和检索的平均代价是O(log2N)。
下面4副图(来源：中国地质大学电子课件)说明了插入顺序为(a)Chicago,(b)Mobile,(c)Toronto,and(d)Buffalo，建立空间K-D树的示例：
应该清楚，这里描述的插入过程中，每个结点将其所在的平面分割成两部分。因比，Chicago将平面上所有结点分成两部分，一部分所有的结点x坐标&#20540;小于35，另一部分结点的x坐标&#20540;大于或等于35。同样Denver将所有x坐标&#20540;小于35的结点以分成两部分，一部分结点的Y坐标&#20540;是小于45，另一部分结点的Y坐标&#20540;大于或等于45。

上述两次实例表明，当查询点的邻域与分割超平面两侧空间交割时，需要查找另一侧子空间，导致检索过程复杂，效率下降。



研究表明N个节点的K维k-d树搜索过程时间复杂度为：tworst=O（kN1-1/k）。
同时，以上为了介绍方便，讨论的是二维或三维情形。但在实际的应用中，如SIFT特征矢量128维，SURF特征矢量64维，维度都比较大，直接利用k-d树快速检索（维数不超过20）的性能急剧下降，几乎接近贪婪线性扫描。假设数据集的维数为D，一般来说要求数据的规模N满足N»2D，才能达到高效的搜索。所以这就引出了一系列对k-d树算法的改进：BBF算法，和一系列M树、VP树、MVP树等高维空间索引树(下文2.6节kd树近邻搜索算法的改进：BBF算法，与2.7节球树、M树、VP树、MVP树)。
咱们顺着上一节的思路，参考统计学习方法一书上的内容，再来总结下kd树的最近邻搜索算法：
如果实例点是随机分布的，那么kd树搜索的平均计算复杂度是O（NlogN），这里的N是训练实例树。所以说，kd树更适用于训练实例数远大于空间维数时的k近邻搜索，当空间维数接近训练实例数时，它的效率会迅速下降，一降降到“解放前”：线性扫描的速度。
也正因为上述k最近邻搜索算法的第4个步骤中的所述：“回退到根结点时，搜索结束”，每个最近邻点的查询比较完成过程最终都要回退到根结点而结束，而导致了许多不必要回溯访问和比较到的结点，这些多余的损耗在高维度数据查找的时候，搜索效率将变得相当之地下，那有什么办法可以改进这个原始的kd树最近邻搜索算法呢？
从上述标准的kd树查询过程可以看出其搜索过程中的“回溯”是由“查询路径”决定的，并没有考虑查询路径上一些数据点本身的一些性质。一个简单的改进思路就是将“查询路径”上的结点进行排序，如按各自分割超平面（也称bin）与查询点的距离排序，也就是说，回溯检查总是从优先级最高（BestBin）的树结点开始。
针对此BBF机制，读者Feng&amp;书童点评道：
如此，就引出了本节要讨论的kd树最近邻搜索算法的改进：BBF（Best-Bin-First）查询算法，它是由发明sift算法的DavidLowe在1997的一篇文章中针对高维数据提出的一种近&#20284;算法，此算法能确保优先检索包含最近邻点可能性较高的空间，此外，BBF机制还设置了一个运行超时限定。采用了BBF查询机制后，kd树便可以有效的扩展到高维数据集上。
伪代码如下图所示（图取自图像局部不变特性特征与描述一书）：

还是以上面的查询（2,4.5）为例，搜索的算法流程为：

咱们来针对上文内容总结回顾下，针对下面这样一棵kd树：

现要找它的最近邻。
通过上文2.5节，总结来说，我们已经知道：
1、为了找到一个给定目标点的最近邻，需要从树的根结点开始向下沿树找出目标点所在的区域，如下图所示，给定目标点，用星号标示，我们&#20284;乎一&#30524;看出，有一个点离目标点最近，因为它落在以目标点为圆心以较小长度为半径的虚线圆内，但为了确定是否可能还村庄一个最近的近邻，我们会先检查叶节点的同胞结点，然叶节点的同胞结点在图中所示的阴影部分，虚线圆并不与之相交，所以确定同胞叶结点不可能包含更近的近邻。

2、于是我们回溯到父节点，并检查父节点的同胞结点，父节点的同胞结点覆盖了图中所有横线X轴上的区域。因为虚线圆与右上方的矩形(KD树把二维平面划分成一个一个矩形)相交...
如上，我们看到，KD树是可用于有效寻找最近邻的一个树结构，但这个树结构其实并不完美，当处理不均匀分布的数据集时便会呈现出一个基本冲突：既邀请树有完美的平衡结构，又要求待查找的区域近&#20284;方形，但不管是近&#20284;方形，还是矩形，甚至正方形，都不是最好的使用形状，因为他们都有角。

什么意思呢？就是说，在上图中，如果黑色的实例点离目标点星点再远一点，那么势必那个虚线圆会如红线所示那样扩大，以致与左上方矩形的右下角相交，既然相交了，那么势必又必须检查这个左上方矩形，而实际上，最近的点离星点的距离很近，检查左上方矩形区域已是多余。于此我们看见，KD树把二维平面划分成一个一个矩形，但矩形区域的角却是个难以处理的问题。
解决的方案就是使用如下图所示的球树：


使用球树找出给定目标点的最近邻方法是，首先自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最靠近的点，这将确定出目标点距离它的最近邻点的一个上限&#20540;，然后跟KD树查找一样，检查同胞结点，如果目标点到同胞结点中心的距离超过同胞结点的半径与当前的上限&#20540;之和，那么同胞结点里不可能存在一个更近的点；否则的话，必须进一步检查位于同胞结点以下的子树。
如下图，目标点还是用一个星表示，黑色点是当前已知的的目标点的最近邻，灰色球里的所有内容将被排除，因为灰色球的中心点离的太远，所以它不可能包含一个更近的点，像这样，递归的向树的根结点进行回溯处理，检查所有可能包含一个更近于当前上限&#20540;的点的球。

球树是自上而下的建立，和KD树一样，根本问题就是要找到一个好的方法将包含数据点集的球分裂成两个，在实践中，不必等到叶子结点只有两个胡数据点时才停止，可以采用和KD树一样的方法，一旦结点上的数据点打到预先设置的最小数量时，便可提前停止建树过程。
也就是上面所述，先从球中选择一个离球的中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个上，然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径。这种方法的优点是分裂一个包含n个殊绝点的球的成本只是随n呈线性增加(注：本小节内容主要来自参考条目19：数据挖掘实用机器学习技术，[新西兰]Ian
H.Witten著，第4章4.7节)。
高维特征向量的距离索引问题是基于内容的图像检索的一项关键技术，目前经常采用的解决办法是首先对高维特征空间做降维处理，然后采用包括四叉树、kd树、R树族等在内的主流多维索引结构，这种方法的出发点是：目前的主流多维索引结构在处理维数较低的情况时具有比较好的效率，但对于维数很高的情况则显得力不从心(即所谓的维数危机)。
实验结果表明当特征空间的维数超过20的时候，效率明显降低，而可视化特征往往采用高维向量描述，一般情况下可以达到10^2的量级，甚至更高。在表示图像可视化特征的高维向量中各维信息的重要程度是不同的，通过降维技术去除属于次要信息的特征向量以及相关性较强的特征向量，从而降低特征空间的维数，这种方法已经得到了一些实际应用。
然而这种方法存在不足之处采用降维技术可能会导致有效信息的损失，尤其不适合于处理特征空间中的特征向量相关性很小的情况。另外主流的多维索引结构大都针对欧氏空间，设计需要利用到欧氏空间的几何性质，而图像的相&#20284;性计算很可能不限于基于欧氏距离。这种情况下人们越来越关注基于距离的度量空间高维索引结构可以直接应用于高维向量相&#20284;性查询问题。
度量空间中对象之间的距离度量只能利用三角不等式性质，而不能利用其他几何性质。向量空间可以看作由实数坐标串组成的特殊度量空间，目前针对度量空间的高维索引问题提出的索引结构有很多种大致可以作如下分类，如下图所示：

读者点评：
更多内容请参见论文1：DISTANCE-BASEDINDEXINGFORHIGH-DIMENSIONALMETRICSPACES，作者：TolgaBozkaya&amp;MeralOzsoyoglu，及论文2：基于度量空间高维索引结构VP-tree及MVP-tree的图像检索，王志强，甘国辉，程起敏。
当然，如果你觉得上述论文还不够满足你胃口的话，这里有一大堆nearestneighboralgorithms相关的论文可供你看：http://scholar.google.com.hk/scholar?q=nearest&#43;neighbor&#43;algorithms&amp;btnG=&amp;hl=zh-CN&amp;as_sdt=0&amp;as_vis=1（其中，这篇可以看下：Spill-Trees，An
investigationofpracticalapproximatenearestneighboralgorithms）。

之前本blog内阐述过图像特征匹配SIFT算法，写过五篇文章，这五篇文章分别为：
不熟悉SIFT算法相关概念的可以看上述几篇文章，这里不再做赘述。与此同时，本文此部分也作为十五个经典算法研究系列里SIFT算法的九之四续。
OK，我们知道，在sift算法中，给定两幅图片图片，若要做特征匹配，一般会先提取出图片中的下列相关属性作为特征点：
函数一的声明：
函数一的实现：
函数二的声明：
函数二的实现：
上文中一直在讲最近邻问题，也就是说只找最近的那唯一一个邻居，但如果现实中需要我们找到k个最近的邻居。该如何做呢？对的，之前blog内曾相近阐述过寻找最小的k个数的问题，显然，寻找k个最近邻与寻找最小的k个数的问题如出一辙。
回忆下寻找k个最小的数中关于构造大顶堆的解决方案：
“寻找最小的k个树，更好的办法是维护k个元素的最大堆，即用容量为k的最大堆存储最先遍历到的k个数，并假设它们即是最小的k个数，建堆费时O（k）后，有k1&lt;k2&lt;...&lt;kmax（kmax设为大顶堆中最大元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，x&lt;kmax，更新堆（用时logk），否则不更新堆。这样下来，总费时O（k&#43;（n-k）*logk）=O（n*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。”
根据上述方法，咱们来写大顶堆最小优先级队列相关代码实现：

原理在上文第二部分已经阐述明白，结合大顶堆找最近的K个近邻思想，相关主体代码如下：
依据上述函数kdtree_bbf_knn从上而下看下来，注意几点：
1、上述函数kdtree_bbf_knn中，explore_to_leaf的代码如下：
2、上述查找函数kdtree_bbf_knn中的参数k可调，代表的是要查找近邻的个数，即numberofneighborstofind，在sift特征匹配中，k一般取2
3、上述函数kdtree_bbf_knn中“bbf_data-&gt;d=descr_dist_sq(feat,tree_feat);//计算两个关键点描述器差平方和”，使用的计算方法是本文第一部分1.2节中所述的欧氏距离。
之前试了下sift&#43;KD&#43;BBF算法，用两幅不同的图片做了下匹配（当然，运行结果显示是不匹配的），效果还不错：http://weibo.com/1580904460/yDmzAEwcV#1348475194313。

“编译的过程中，直接用的VS2010&#43;opencv（并没下gsl）。2012.09.24”。....
本文完整源码有pudn帐号的朋友可以前去这里下载：http://www.pudn.com/downloads340/sourcecode/graph/texture_mapping/detail1486667.html（没有pudn账号的同学请加群：169056165，至群共享下载，验证信息：sift）。感谢诸位。

