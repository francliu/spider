
机器学习算法与Python实践这个系列主要是参考《机器学习实战》这本书。因为自己想学习Python，然后也想对一些机器学习算法加深下了解，所以就想通过Python来实现几个比较常用的机器学习算法。恰好遇见这本同样定位的书籍，所以就参考这本书的过程来学习了。
机器学习中有两类的大问题，一个是分类，一个是聚类。分类是根据一些给定的已知类别标号的样本，训练某种学习机器，使它能够对未知类别的样本进行分类。这属于supervisedlearning（监督学习）。而聚类指事先并不知道任何样本的类别标号，希望通过某种算法来把一组未知类别的样本划分成若干类别，这在机器学习中被称作unsupervisedlearning（无监督学习）。在本文中，我们关注其中一个比较简单的聚类算法：k-means算法。

一、k-means算法
通常，人们根据样本间的某种距离或者相&#20284;性来定义聚类，即把相&#20284;的（或距离近的）样本聚为同一类，而把不相&#20284;的（或距离远的）样本归在其他类。
我们以一个二维的例子来说明下聚类的目的。如下图左所示，假设我们的n个样本点分布在图中所示的二维空间。从数据点的大致形状可以看出它们大致聚为三个cluster，其中两个紧凑一些，剩下那个松散一些。我们的目的是为这些数据分组，以便能区分出属于不同的簇的数据，如果按照分组给它们标上不同的颜色，就是像下图右边的图那样：
如果人可以看到像上图那样的数据分布，就可以轻松进行聚类。但我们怎么教会计算机按照我们的思维去做同样的事情呢？这里就介绍个集简单和经典于一身的k-means算法。
k-means算法是一种很常见的聚类算法，它的基本思想是：通过迭代寻找k个聚类的一种划分方案，使得用这k个聚类的均&#20540;来代表相应各类样本时所得的总体误差最小。
k-means算法的基础是最小误差平方和准则。其代价函数是：
式中，μc(i)表示第i个聚类的均&#20540;。我们希望代价函数最小，直观的来说，各类内的样本越相&#20284;，其与该类均&#20540;间的误差平方越小，对所有类所得到的误差平方求和，即可验证分为k类时，各聚类是否是最优的。
上式的代价函数无法用解析的方法最小化，只能有迭代的方法。k-means算法是将样本聚类成k个簇（cluster），其中k是用户给定的，其求解过程非常直观简单，具体算法描述如下：
1、随机选取k个聚类质心点
2、重复下面过程直到收敛
{
对于每一个样例i，计算其应该属于的类：
对于每一个类j，重新计算该类的质心：
}
下图展示了对n个样本点进行K-means聚类的效果，这里k取2。
其伪代码如下：
********************************************************************
创建k个点作为初始的质心点（随机选择）
当任意一个点的簇分配结果发生改变时
对数据集中的每一个数据点
对每一个质心
计算质心与数据点的距离
将数据点分配到距离最近的簇
对每一个簇，计算簇中所有点的均&#20540;，并将均&#20540;作为质心
********************************************************************

二、Python实现
我使用的Python是2.7.5版本的。附加的库有Numpy和Matplotlib。具体的安装和配置见前面的博文。在代码中已经有了比较详细的注释了。不知道有没有错误的地方，如果有，还望大家指正（每次的运行结果都有可能不同）。里面我写了个可视化结果的函数，但只能在二维的数据上面使用。直接贴代码：
kmeans.py
三、测试结果
测试数据是二维的，共80个样本。有4个类。如下：
testSet.txt

测试代码：
test_kmeans.py

运行的前后结果是：
不同的类用不同的颜色来表示，其中的大菱形是对应类的均&#20540;质心点。

四、算法分析
k-means算法比较简单，但也有几个比较大的缺点：
（1）k&#20540;的选择是用户指定的，不同的k得到的结果会有挺大的不同，如下图所示，左边是k=3的结果，这个就太稀疏了，蓝色的那个簇其实是可以再划分成两个簇的。而右图是k=5的结果，可以看到红色菱形和蓝色菱形这两个簇应该是可以合并成一个簇的：
（2）对k个初始质心的选择比较敏感，容易陷入局部最小&#20540;。例如，我们上面的算法运行的时候，有可能会得到不同的结果，如下面这两种情况。K-means也是收敛了，只是收敛到了局部最小&#20540;：
（3）存在局限性，如下面这种非球状的数据分布就搞不定了：
（4）数据库比较大的时候，收敛会比较慢。
k-means老早就出现在江湖了。所以以上的这些不足也被世人的目光敏锐的捕捉到，并融入世人的智慧进行了某种程度上的改良。例如问题（1）对k的选择可以先用一些算法分析数据的分布，如重心和密度等，然后选择合适的k。而对问题（2），有人提出了另一个成为二分k均&#20540;（bisectingk-means）算法，它对初始的k个质心的选择就不太敏感，这个算法我们下一个博文再分析和实现。

五、参考文献
[1]
K-means聚类算法
[2]
漫谈Clustering(1):k-means

机器学习算法与Python实践这个系列主要是参考《机器学习实战》这本书。因为自己想学习Python，然后也想对一些机器学习算法加深下了解，所以就想通过Python来实现几个比较常用的机器学习算法。恰好遇见这本同样定位的书籍，所以就参考这本书的过程来学习了。
机器学习中有两类的大问题，一个是分类，一个是聚类。分类是根据一些给定的已知类别标号的样本，训练某种学习机器，使它能够对未知类别的样本进行分类。这属于supervisedlearning（监督学习）。而聚类指事先并不知道任何样本的类别标号，希望通过某种算法来把一组未知类别的样本划分成若干类别，这在机器学习中被称作unsupervisedlearning（无监督学习）。在本文中，我们关注其中一个比较简单的聚类算法：k-means算法。

一、k-means算法
通常，人们根据样本间的某种距离或者相&#20284;性来定义聚类，即把相&#20284;的（或距离近的）样本聚为同一类，而把不相&#20284;的（或距离远的）样本归在其他类。
我们以一个二维的例子来说明下聚类的目的。如下图左所示，假设我们的n个样本点分布在图中所示的二维空间。从数据点的大致形状可以看出它们大致聚为三个cluster，其中两个紧凑一些，剩下那个松散一些。我们的目的是为这些数据分组，以便能区分出属于不同的簇的数据，如果按照分组给它们标上不同的颜色，就是像下图右边的图那样：
如果人可以看到像上图那样的数据分布，就可以轻松进行聚类。但我们怎么教会计算机按照我们的思维去做同样的事情呢？这里就介绍个集简单和经典于一身的k-means算法。
k-means算法是一种很常见的聚类算法，它的基本思想是：通过迭代寻找k个聚类的一种划分方案，使得用这k个聚类的均&#20540;来代表相应各类样本时所得的总体误差最小。
k-means算法的基础是最小误差平方和准则。其代价函数是：
式中，μc(i)表示第i个聚类的均&#20540;。我们希望代价函数最小，直观的来说，各类内的样本越相&#20284;，其与该类均&#20540;间的误差平方越小，对所有类所得到的误差平方求和，即可验证分为k类时，各聚类是否是最优的。
上式的代价函数无法用解析的方法最小化，只能有迭代的方法。k-means算法是将样本聚类成k个簇（cluster），其中k是用户给定的，其求解过程非常直观简单，具体算法描述如下：
1、随机选取k个聚类质心点
2、重复下面过程直到收敛
{
对于每一个样例i，计算其应该属于的类：
对于每一个类j，重新计算该类的质心：
}
下图展示了对n个样本点进行K-means聚类的效果，这里k取2。
其伪代码如下：
********************************************************************
创建k个点作为初始的质心点（随机选择）
当任意一个点的簇分配结果发生改变时
对数据集中的每一个数据点
对每一个质心
计算质心与数据点的距离
将数据点分配到距离最近的簇
对每一个簇，计算簇中所有点的均&#20540;，并将均&#20540;作为质心
********************************************************************

二、Python实现
我使用的Python是2.7.5版本的。附加的库有Numpy和Matplotlib。具体的安装和配置见前面的博文。在代码中已经有了比较详细的注释了。不知道有没有错误的地方，如果有，还望大家指正（每次的运行结果都有可能不同）。里面我写了个可视化结果的函数，但只能在二维的数据上面使用。直接贴代码：
kmeans.py
三、测试结果
测试数据是二维的，共80个样本。有4个类。如下：
testSet.txt

测试代码：
test_kmeans.py

运行的前后结果是：
不同的类用不同的颜色来表示，其中的大菱形是对应类的均&#20540;质心点。

四、算法分析
k-means算法比较简单，但也有几个比较大的缺点：
（1）k&#20540;的选择是用户指定的，不同的k得到的结果会有挺大的不同，如下图所示，左边是k=3的结果，这个就太稀疏了，蓝色的那个簇其实是可以再划分成两个簇的。而右图是k=5的结果，可以看到红色菱形和蓝色菱形这两个簇应该是可以合并成一个簇的：
（2）对k个初始质心的选择比较敏感，容易陷入局部最小&#20540;。例如，我们上面的算法运行的时候，有可能会得到不同的结果，如下面这两种情况。K-means也是收敛了，只是收敛到了局部最小&#20540;：
（3）存在局限性，如下面这种非球状的数据分布就搞不定了：
（4）数据库比较大的时候，收敛会比较慢。
k-means老早就出现在江湖了。所以以上的这些不足也被世人的目光敏锐的捕捉到，并融入世人的智慧进行了某种程度上的改良。例如问题（1）对k的选择可以先用一些算法分析数据的分布，如重心和密度等，然后选择合适的k。而对问题（2），有人提出了另一个成为二分k均&#20540;（bisectingk-means）算法，它对初始的k个质心的选择就不太敏感，这个算法我们下一个博文再分析和实现。

五、参考文献
[1]
K-means聚类算法
[2]
漫谈Clustering(1):k-means
