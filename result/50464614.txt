
pd.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,sort=False,suffixes=('_x','_y'),copy=True)用于通过一个或多个键将两个数据集的行连接起来，类&#20284;于SQL中的JOIN。该函数的典型应用场景是，针对同一个主键存在两张包含不同字段的表，现在我们想把他们整合到一张表里。在此典型情况下，结果集的行数并没有增加，列数则为两个元数据的列数和减去连接键的数量。
on=None用于显示指定列名（键名），如果该列在两个对象上的列名不同，则可以通过left_on=None,right_on=None来分别指定。或者想直接使用行索引作为连接键的话，就将
left_index=False,right_index=False设为True。
how='inner'参数指的是当左右两个对象中存在不重合的键时，取结果的方式：inner代表交集；outer代表并集；left和right分别为取一边。
suffixes=('_x','_y')指的是当左右对象中存在除连接键外的同名列时，结果集中的区分方式，可以各加一个小尾巴。
对于多对多连接，结果采用的是行的笛卡尔积。
示例：
DataFrame还有一个方法：.join(self,other,on=None,how='left',lsuffix='',rsuffix='',sort=False)，它能更方便地实现按索引合并。它还可以用于合并多个带有相同或相&#20284;索引的DataFrame对象，而不管他们之间有没有重叠的列。&#20540;得注意的是它的参数里
lsuffix='',rsuffix=''并没有给出默认&#20540;，所以当你的对象中有列重叠（columnsoverlap）时需要显示指定suffix参数，否则会报ValueError：

merge算是一种整合的话，轴向连接pd.concat()就是单纯地把两个表拼在一起，这个过程也被称作连接（concatenation）、绑定（binding）或堆叠（stacking）。
因此可以想见，这个函数的关键参数应该是axis，用于指定连接的轴向。在默认的axis=0情况下，pd.concat([obj1,obj2])函数的效果与
obj1.append(obj2)是相同的；而在axis=1的情况下，pd.concat([df1,df2],axis=1)的效果与
pd.merge(df1,df2,left_index=True,right_index=True,how='outer')是相同的。可以理解为concat函数使用索引作为“连接键”。
本函数的全部参数为：pd.concat(objs,axis=0,join='outer',join_axes=None,ignore_index=False,keys=None,levels=None,names=None,verify_integrity=False)。
objs就是需要连接的对象集合，一般是列表或字典；axis=0是连接轴向
join='outer'参数作用于当另一条轴的index不重叠的时候，只有'inner'和
'outer'可选（顺带展示ignore_index=True的用法）：
join_axes=None参数用于详细制定其他轴上使用的索引，优先级可以覆盖join参数，join_axes的类型是一个列表，其中的元素为其他轴的index。比如上例两条命令等价于这样：pd.concat([df1,df2],join_axes=[['a','b']])、
pd.concat([df1,df2],join_axes=[['a']])
keys=None参数的作用是在结果集中对源数据进行区分。前例中可以看到，结果集中的项无法区分来源，因此使用一个列表型的keys参数可以在连接轴上创建一个层次化索引；另一个隐式使用keys参数的方法是传入objs参数时使用字典，字典的键就会被当做keys。
levels=None和names=None参数与keys参数有关，这里pass；verify_integrity=False参数用于检查结果对象新连接轴上的索引是否有重复项，有的话引发ValueError，可以看到这个参数的作用与
ignore_index是互斥的。

obj.combine_first(other)方法的作用是使用other中的数据去填补obj中的NA&#20540;，就像打补丁。而且可以自动对齐。

有许多用于重新排列表&#26684;数据的基础运算。这些函数称为重塑（reshape）或轴向旋转（pivot）运算。
层次化索引为DataFrame数据的重排任务提供了一种具有良好一致性的方式。重塑层次化索引通过以下两个方法完成：
示例：
可见，如果是普通的多列DataFrame，调用一次stack后就会变成Series了。
默认情况下，unstack操作的是最内层（stack亦如此）。传入分层级别的编号或name即可对其他级别进行操作。

时间序列数据通常都是以所谓的“长&#26684;式”（long）或“堆叠&#26684;式”（stacked）存储在数据库或CSV中的：
这个item其实只包含三个字段——realgdp、infl和unemp，但每一个字段都单独存储为一行。这样做的好处是在数据库中维护了一个动态的item字段，以后如果item的项有增删的话，也不必改变表结构。但这种做法的冗余信息过多，而且操作起来很麻烦，需要额外输入很多命令，因此在处理数据前先将其“展开”为“宽&#26684;式”就显得很有必要。
这项任务其实在上一节中就已经给出了解决方法，不过本节要介绍的是一种“快捷方式”——obj.pivot(index=None,columns=None,values=None)方法。三个参数都应是来自obj的列名，或列对象。分别用于指定结果对象的index、columns和values属性。

除了前面介绍的数据重排外，另一种重要操作是过滤、清理以及其他的转换工作
移除重复数据操作有两个方法可用
这两个方法默认都会检查所有的列，如果想仅针对某一（些）列进行检查的话，可以传入cols参数，指定需要检查的列。
方法默认将第一个出现的&#20540;保留，还有一个take_last=False参数，可将其改为True以保留最后的&#20540;。
Series或DataFrame的列都可以调用一个.map()方法。该方法接受一个函数或字典作为参数，并将之应用于对象的每一个元素，最后返回一个包含所有结果的Series。
一个例子写了两遍是为了展示map方法的嵌套用法。
fillna方法填充缺失&#20540;可以看做&#20540;替换的一种特殊情况，map也可以用来修改对象的数据子集，而.replace(to_replace=None,value=None,inplace=False,limit=None,regex=False,method='pad',axis=None)方法则提供了实现该功能的一种更简单、更灵活的方式。
to_replace参数可以是：str,regex,list,dict,Series,numeric,orNone；value参数可以是：scalar,dict,list,str,regex,defaultNone。其他参数的特殊用法请使用help查看。

前面应该提到过，pandas对象的index参数是不可变（immutable）的，即不可以直接对其元素进行赋&#20540;操作。但你却可以对其使用
obj.index.map()方法。
也可以直接对数组对象调用obj.rename(index=None,columns=None)方法。这里的index和columns参数并不是index对象，而是一个函数或字典：

为了便于分析，连续数据常常被离散化或拆分为“面元”（bin）。这个过程要使用到pandas的cut函数：
cut(x,bins,right=True,labels=None,retbins=False,precision=3,include_lowest=False)
核心参数为x和bins，x为被切对象，应当是个一维的类数组结构；bins参数可以是序列、整数或标量。
序列：按序列的元素间隔划分x，返回x各个元素的分组情况
整数：以x的上下界等长划分，可用precision参数调节精度。
right=True参数用于控制序列型bins的边界，默认为右包含。labels参数可以给bins添加代号。
最后我们来看一下cut函数返回的这个对象：
Categorical对象是一个枚举型的序列对象，它的可选&#20540;都显示在levels属性里。
另一个pd.qcut()函数与cut类&#20284;，但它可以根据样本的分位数对数据进行面元划分：
cut与qcut的更多用法会在数据聚合与分组篇中提及。
异常&#20540;（outlier）的过滤或变换运算在很大程度上就是数组运算。如下一个（1000,4）的标准正态分布数组
假设要找出某一列中绝对&#20540;大小超过3的项：
要选出全部含有“绝对&#20540;超过3的&#20540;”的行，可以利用布尔型索引和any方法：
以下命令会将data的&#20540;全部限制在[-3,3]之间，通过将异常&#20540;替换为-3和3的方式。
np.sign()函数可以返回一个由-1和1组成的数组，表示原始&#20540;的符号。
随机采样的基本思路是：先利用np.random模块随机生成一个需要的索引，然后利用这个索引去源数据里过滤取&#20540;。随机采样的两个常用函数为
np.random.randint(start,end,size)这个函数一般用于实现“可重取”的随机采样，因为返回的数组中的元素可重复，而且size可变
np.random.permutation(x)函数用于随机排列一个序列类型。x参数接受整数或类序列类型，实际处理过程中都是按序列来处理的——整型x会当做range(x)来处理。本函数会随机重排（shuffle）接收到的序列参数并返回一个新结果，显然这是一个“不可重取”的抽样，且size最大即为len(x)。
因为sampler是一个数组类型，所以用它在源数据中取&#20540;的方式有很多↑，如果不想全部取样的话，给sampler加个切片就可以了。
pd.get_dummies(data,prefix=None,prefix_sep='_',dummy_na=False)函数可用来将分类变量（Categoricalvariable）转换为“哑变量矩阵”（dummymatrix）或称“指标矩阵”（indicatormatrix）。更加便捷的是，data参数并不限于categorical类型，而是可以直接使用一个类Series对象，比如DataFrame的列。本函数返回的是一个以
data元素为列名的1、0矩阵。
将本函数直接应用于DataFrame的列上，再与原数据剩余部分连接：

在对字符串元素进行规整化操作时，使用.map()方法的一个弊端是需要小心绕过NA&#20540;。为了解决这个问题，Series直接提供了一些能够跳过NA&#20540;的字符串操作方法，全部通过
ser.str.xxx()来访问。这些方法一般也都支持正则表达式。
有两个办法可以实现矢量化的元素获取操作：要么使用str.get，要么在str属性上使用索引。
其他一些矢量化的字符串方法有：

pd.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,sort=False,suffixes=('_x','_y'),copy=True)用于通过一个或多个键将两个数据集的行连接起来，类&#20284;于SQL中的JOIN。该函数的典型应用场景是，针对同一个主键存在两张包含不同字段的表，现在我们想把他们整合到一张表里。在此典型情况下，结果集的行数并没有增加，列数则为两个元数据的列数和减去连接键的数量。
on=None用于显示指定列名（键名），如果该列在两个对象上的列名不同，则可以通过left_on=None,right_on=None来分别指定。或者想直接使用行索引作为连接键的话，就将
left_index=False,right_index=False设为True。
how='inner'参数指的是当左右两个对象中存在不重合的键时，取结果的方式：inner代表交集；outer代表并集；left和right分别为取一边。
suffixes=('_x','_y')指的是当左右对象中存在除连接键外的同名列时，结果集中的区分方式，可以各加一个小尾巴。
对于多对多连接，结果采用的是行的笛卡尔积。
示例：
DataFrame还有一个方法：.join(self,other,on=None,how='left',lsuffix='',rsuffix='',sort=False)，它能更方便地实现按索引合并。它还可以用于合并多个带有相同或相&#20284;索引的DataFrame对象，而不管他们之间有没有重叠的列。&#20540;得注意的是它的参数里
lsuffix='',rsuffix=''并没有给出默认&#20540;，所以当你的对象中有列重叠（columnsoverlap）时需要显示指定suffix参数，否则会报ValueError：

merge算是一种整合的话，轴向连接pd.concat()就是单纯地把两个表拼在一起，这个过程也被称作连接（concatenation）、绑定（binding）或堆叠（stacking）。
因此可以想见，这个函数的关键参数应该是axis，用于指定连接的轴向。在默认的axis=0情况下，pd.concat([obj1,obj2])函数的效果与
obj1.append(obj2)是相同的；而在axis=1的情况下，pd.concat([df1,df2],axis=1)的效果与
pd.merge(df1,df2,left_index=True,right_index=True,how='outer')是相同的。可以理解为concat函数使用索引作为“连接键”。
本函数的全部参数为：pd.concat(objs,axis=0,join='outer',join_axes=None,ignore_index=False,keys=None,levels=None,names=None,verify_integrity=False)。
objs就是需要连接的对象集合，一般是列表或字典；axis=0是连接轴向
join='outer'参数作用于当另一条轴的index不重叠的时候，只有'inner'和
'outer'可选（顺带展示ignore_index=True的用法）：
join_axes=None参数用于详细制定其他轴上使用的索引，优先级可以覆盖join参数，join_axes的类型是一个列表，其中的元素为其他轴的index。比如上例两条命令等价于这样：pd.concat([df1,df2],join_axes=[['a','b']])、
pd.concat([df1,df2],join_axes=[['a']])
keys=None参数的作用是在结果集中对源数据进行区分。前例中可以看到，结果集中的项无法区分来源，因此使用一个列表型的keys参数可以在连接轴上创建一个层次化索引；另一个隐式使用keys参数的方法是传入objs参数时使用字典，字典的键就会被当做keys。
levels=None和names=None参数与keys参数有关，这里pass；verify_integrity=False参数用于检查结果对象新连接轴上的索引是否有重复项，有的话引发ValueError，可以看到这个参数的作用与
ignore_index是互斥的。

obj.combine_first(other)方法的作用是使用other中的数据去填补obj中的NA&#20540;，就像打补丁。而且可以自动对齐。

有许多用于重新排列表&#26684;数据的基础运算。这些函数称为重塑（reshape）或轴向旋转（pivot）运算。
层次化索引为DataFrame数据的重排任务提供了一种具有良好一致性的方式。重塑层次化索引通过以下两个方法完成：
示例：
可见，如果是普通的多列DataFrame，调用一次stack后就会变成Series了。
默认情况下，unstack操作的是最内层（stack亦如此）。传入分层级别的编号或name即可对其他级别进行操作。

时间序列数据通常都是以所谓的“长&#26684;式”（long）或“堆叠&#26684;式”（stacked）存储在数据库或CSV中的：
这个item其实只包含三个字段——realgdp、infl和unemp，但每一个字段都单独存储为一行。这样做的好处是在数据库中维护了一个动态的item字段，以后如果item的项有增删的话，也不必改变表结构。但这种做法的冗余信息过多，而且操作起来很麻烦，需要额外输入很多命令，因此在处理数据前先将其“展开”为“宽&#26684;式”就显得很有必要。
这项任务其实在上一节中就已经给出了解决方法，不过本节要介绍的是一种“快捷方式”——obj.pivot(index=None,columns=None,values=None)方法。三个参数都应是来自obj的列名，或列对象。分别用于指定结果对象的index、columns和values属性。

除了前面介绍的数据重排外，另一种重要操作是过滤、清理以及其他的转换工作
移除重复数据操作有两个方法可用
这两个方法默认都会检查所有的列，如果想仅针对某一（些）列进行检查的话，可以传入cols参数，指定需要检查的列。
方法默认将第一个出现的&#20540;保留，还有一个take_last=False参数，可将其改为True以保留最后的&#20540;。
Series或DataFrame的列都可以调用一个.map()方法。该方法接受一个函数或字典作为参数，并将之应用于对象的每一个元素，最后返回一个包含所有结果的Series。
一个例子写了两遍是为了展示map方法的嵌套用法。
fillna方法填充缺失&#20540;可以看做&#20540;替换的一种特殊情况，map也可以用来修改对象的数据子集，而.replace(to_replace=None,value=None,inplace=False,limit=None,regex=False,method='pad',axis=None)方法则提供了实现该功能的一种更简单、更灵活的方式。
to_replace参数可以是：str,regex,list,dict,Series,numeric,orNone；value参数可以是：scalar,dict,list,str,regex,defaultNone。其他参数的特殊用法请使用help查看。

前面应该提到过，pandas对象的index参数是不可变（immutable）的，即不可以直接对其元素进行赋&#20540;操作。但你却可以对其使用
obj.index.map()方法。
也可以直接对数组对象调用obj.rename(index=None,columns=None)方法。这里的index和columns参数并不是index对象，而是一个函数或字典：

为了便于分析，连续数据常常被离散化或拆分为“面元”（bin）。这个过程要使用到pandas的cut函数：
cut(x,bins,right=True,labels=None,retbins=False,precision=3,include_lowest=False)
核心参数为x和bins，x为被切对象，应当是个一维的类数组结构；bins参数可以是序列、整数或标量。
序列：按序列的元素间隔划分x，返回x各个元素的分组情况
整数：以x的上下界等长划分，可用precision参数调节精度。
right=True参数用于控制序列型bins的边界，默认为右包含。labels参数可以给bins添加代号。
最后我们来看一下cut函数返回的这个对象：
Categorical对象是一个枚举型的序列对象，它的可选&#20540;都显示在levels属性里。
另一个pd.qcut()函数与cut类&#20284;，但它可以根据样本的分位数对数据进行面元划分：
cut与qcut的更多用法会在数据聚合与分组篇中提及。
异常&#20540;（outlier）的过滤或变换运算在很大程度上就是数组运算。如下一个（1000,4）的标准正态分布数组
假设要找出某一列中绝对&#20540;大小超过3的项：
要选出全部含有“绝对&#20540;超过3的&#20540;”的行，可以利用布尔型索引和any方法：
以下命令会将data的&#20540;全部限制在[-3,3]之间，通过将异常&#20540;替换为-3和3的方式。
np.sign()函数可以返回一个由-1和1组成的数组，表示原始&#20540;的符号。
随机采样的基本思路是：先利用np.random模块随机生成一个需要的索引，然后利用这个索引去源数据里过滤取&#20540;。随机采样的两个常用函数为
np.random.randint(start,end,size)这个函数一般用于实现“可重取”的随机采样，因为返回的数组中的元素可重复，而且size可变
np.random.permutation(x)函数用于随机排列一个序列类型。x参数接受整数或类序列类型，实际处理过程中都是按序列来处理的——整型x会当做range(x)来处理。本函数会随机重排（shuffle）接收到的序列参数并返回一个新结果，显然这是一个“不可重取”的抽样，且size最大即为len(x)。
因为sampler是一个数组类型，所以用它在源数据中取&#20540;的方式有很多↑，如果不想全部取样的话，给sampler加个切片就可以了。
pd.get_dummies(data,prefix=None,prefix_sep='_',dummy_na=False)函数可用来将分类变量（Categoricalvariable）转换为“哑变量矩阵”（dummymatrix）或称“指标矩阵”（indicatormatrix）。更加便捷的是，data参数并不限于categorical类型，而是可以直接使用一个类Series对象，比如DataFrame的列。本函数返回的是一个以
data元素为列名的1、0矩阵。
将本函数直接应用于DataFrame的列上，再与原数据剩余部分连接：

在对字符串元素进行规整化操作时，使用.map()方法的一个弊端是需要小心绕过NA&#20540;。为了解决这个问题，Series直接提供了一些能够跳过NA&#20540;的字符串操作方法，全部通过
ser.str.xxx()来访问。这些方法一般也都支持正则表达式。
有两个办法可以实现矢量化的元素获取操作：要么使用str.get，要么在str属性上使用索引。
其他一些矢量化的字符串方法有：
