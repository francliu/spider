对于Adaboost，可以说是久闻大名，据说在DeepLearning出来之前，SVM和Adaboost是效果最好的两个算法，而Adaboost是提升树(boostingtree)，所谓“提升树”就是把“弱学习算法”提升(boost)为“强学习算法”(语自《统计学习方法》)，而其中最具代表性的也就是Adaboost了，貌&#20284;Adaboost的结构还和NeuralNetwork有几分神&#20284;，我倒没有深究过，不知道是不是有什么干货

（fromPRML）
这就是Adaboost的结构，最后的分类器YM是由数个弱分类器（weakclassifier）组合而成的,相当于最后m个弱分类器来投票决定分类，而且每个弱分类器的“话语权”α不一样。
这里阐述下算法的具体过程：
a).训练弱分类器ym()，使其最小化权重误差函数（weightederrorfunction）：

b)接下来计算该弱分类器的话语权α：

c)更新权重：

其中Zm：

是规范化因子，使所有w的和为1。(这里公式稍微有点乱)

对于Adaboost，可以说是久闻大名，据说在DeepLearning出来之前，SVM和Adaboost是效果最好的两个算法，而Adaboost是提升树(boostingtree)，所谓“提升树”就是把“弱学习算法”提升(boost)为“强学习算法”(语自《统计学习方法》)，而其中最具代表性的也就是Adaboost了，貌&#20284;Adaboost的结构还和NeuralNetwork有几分神&#20284;，我倒没有深究过，不知道是不是有什么干货

（fromPRML）
这就是Adaboost的结构，最后的分类器YM是由数个弱分类器（weakclassifier）组合而成的,相当于最后m个弱分类器来投票决定分类，而且每个弱分类器的“话语权”α不一样。
这里阐述下算法的具体过程：
a).训练弱分类器ym()，使其最小化权重误差函数（weightederrorfunction）：

b)接下来计算该弱分类器的话语权α：

c)更新权重：

其中Zm：

是规范化因子，使所有w的和为1。(这里公式稍微有点乱)

